{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, sep='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n",
    "train.shape\n",
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "example1 = BeautifulSoup(train['review'][0])\n",
    "print example1.get_text()\n",
    "print train['review'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again', u'maybe', u'i', u'just', u'want', u'to', u'get', u'a', u'certain', u'insight', u'into', u'this', u'guy', u'who', u'i', u'thought', u'was', u'really', u'cool', u'in', u'the', u'eighties', u'just', u'to', u'maybe', u'make', u'up', u'my', u'mind', u'whether', u'he', u'is', u'guilty', u'or', u'innocent', u'moonwalker', u'is', u'part', u'biography', u'part', u'feature', u'film', u'which', u'i', u'remember', u'going', u'to', u'see', u'at', u'the', u'cinema', u'when', u'it', u'was', u'originally', u'released', u'some', u'of', u'it', u'has', u'subtle', u'messages', u'about', u'mj', u's', u'feeling', u'towards', u'the', u'press', u'and', u'also', u'the', u'obvious', u'message', u'of', u'drugs', u'are', u'bad', u'm', u'kay', u'visually', u'impressive', u'but', u'of', u'course', u'this', u'is', u'all', u'about', u'michael', u'jackson', u'so', u'unless', u'you', u'remotely', u'like', u'mj', u'in', u'anyway', u'then', u'you', u'are', u'going', u'to', u'hate', u'this', u'and', u'find', u'it', u'boring', u'some', u'may', u'call', u'mj', u'an', u'egotist', u'for', u'consenting', u'to', u'the', u'making', u'of', u'this', u'movie', u'but', u'mj', u'and', u'most', u'of', u'his', u'fans', u'would', u'say', u'that', u'he', u'made', u'it', u'for', u'the', u'fans', u'which', u'if', u'true', u'is', u'really', u'nice', u'of', u'him', u'the', u'actual', u'feature', u'film', u'bit', u'when', u'it', u'finally', u'starts', u'is', u'only', u'on', u'for', u'minutes', u'or', u'so', u'excluding', u'the', u'smooth', u'criminal', u'sequence', u'and', u'joe', u'pesci', u'is', u'convincing', u'as', u'a', u'psychopathic', u'all', u'powerful', u'drug', u'lord', u'why', u'he', u'wants', u'mj', u'dead', u'so', u'bad', u'is', u'beyond', u'me', u'because', u'mj', u'overheard', u'his', u'plans', u'nah', u'joe', u'pesci', u's', u'character', u'ranted', u'that', u'he', u'wanted', u'people', u'to', u'know', u'it', u'is', u'he', u'who', u'is', u'supplying', u'drugs', u'etc', u'so', u'i', u'dunno', u'maybe', u'he', u'just', u'hates', u'mj', u's', u'music', u'lots', u'of', u'cool', u'things', u'in', u'this', u'like', u'mj', u'turning', u'into', u'a', u'car', u'and', u'a', u'robot', u'and', u'the', u'whole', u'speed', u'demon', u'sequence', u'also', u'the', u'director', u'must', u'have', u'had', u'the', u'patience', u'of', u'a', u'saint', u'when', u'it', u'came', u'to', u'filming', u'the', u'kiddy', u'bad', u'sequence', u'as', u'usually', u'directors', u'hate', u'working', u'with', u'one', u'kid', u'let', u'alone', u'a', u'whole', u'bunch', u'of', u'them', u'performing', u'a', u'complex', u'dance', u'scene', u'bottom', u'line', u'this', u'movie', u'is', u'for', u'people', u'who', u'like', u'mj', u'on', u'one', u'level', u'or', u'another', u'which', u'i', u'think', u'is', u'most', u'people', u'if', u'not', u'then', u'stay', u'away', u'it', u'does', u'try', u'and', u'give', u'off', u'a', u'wholesome', u'message', u'and', u'ironically', u'mj', u's', u'bestest', u'buddy', u'in', u'this', u'movie', u'is', u'a', u'girl', u'michael', u'jackson', u'is', u'truly', u'one', u'of', u'the', u'most', u'talented', u'people', u'ever', u'to', u'grace', u'this', u'planet', u'but', u'is', u'he', u'guilty', u'well', u'with', u'all', u'the', u'attention', u'i', u've', u'gave', u'this', u'subject', u'hmmm', u'well', u'i', u'don', u't', u'know', u'because', u'people', u'can', u'be', u'different', u'behind', u'closed', u'doors', u'i', u'know', u'this', u'for', u'a', u'fact', u'he', u'is', u'either', u'an', u'extremely', u'nice', u'but', u'stupid', u'guy', u'or', u'one', u'of', u'the', u'most', u'sickest', u'liars', u'i', u'hope', u'he', u'is', u'not', u'the', u'latter']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation and numbers\n",
    "# tokenize\n",
    "import re\n",
    "letters1 = re.sub(r'[^a-zA-Z]', ' ', example1.get_text())\n",
    "letters1 = letters1.lower()\n",
    "letters1 = letters1.split()\n",
    "print letters1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "[u'stuff', u'going', u'moment', u'mj', u've', u'started', u'listening', u'music', u'watching', u'odd', u'documentary', u'watched', u'wiz', u'watched', u'moonwalker', u'maybe', u'want', u'get', u'certain', u'insight', u'guy', u'thought', u'really', u'cool', u'eighties', u'maybe', u'make', u'mind', u'whether', u'guilty', u'innocent', u'moonwalker', u'part', u'biography', u'part', u'feature', u'film', u'remember', u'going', u'see', u'cinema', u'originally', u'released', u'subtle', u'messages', u'mj', u'feeling', u'towards', u'press', u'also', u'obvious', u'message', u'drugs', u'bad', u'm', u'kay', u'visually', u'impressive', u'course', u'michael', u'jackson', u'unless', u'remotely', u'like', u'mj', u'anyway', u'going', u'hate', u'find', u'boring', u'may', u'call', u'mj', u'egotist', u'consenting', u'making', u'movie', u'mj', u'fans', u'would', u'say', u'made', u'fans', u'true', u'really', u'nice', u'actual', u'feature', u'film', u'bit', u'finally', u'starts', u'minutes', u'excluding', u'smooth', u'criminal', u'sequence', u'joe', u'pesci', u'convincing', u'psychopathic', u'powerful', u'drug', u'lord', u'wants', u'mj', u'dead', u'bad', u'beyond', u'mj', u'overheard', u'plans', u'nah', u'joe', u'pesci', u'character', u'ranted', u'wanted', u'people', u'know', u'supplying', u'drugs', u'etc', u'dunno', u'maybe', u'hates', u'mj', u'music', u'lots', u'cool', u'things', u'like', u'mj', u'turning', u'car', u'robot', u'whole', u'speed', u'demon', u'sequence', u'also', u'director', u'must', u'patience', u'saint', u'came', u'filming', u'kiddy', u'bad', u'sequence', u'usually', u'directors', u'hate', u'working', u'one', u'kid', u'let', u'alone', u'whole', u'bunch', u'performing', u'complex', u'dance', u'scene', u'bottom', u'line', u'movie', u'people', u'like', u'mj', u'one', u'level', u'another', u'think', u'people', u'stay', u'away', u'try', u'give', u'wholesome', u'message', u'ironically', u'mj', u'bestest', u'buddy', u'movie', u'girl', u'michael', u'jackson', u'truly', u'one', u'talented', u'people', u'ever', u'grace', u'planet', u'guilty', u'well', u'attention', u've', u'gave', u'subject', u'hmmm', u'well', u'know', u'people', u'different', u'behind', u'closed', u'doors', u'know', u'fact', u'either', u'extremely', u'nice', u'stupid', u'guy', u'one', u'sickest', u'liars', u'hope', u'latter']\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print len(stopwords.words('english'))\n",
    "#In Python, searching a set is much faster than searching\n",
    "#   a list, so convert the stop words to a set\n",
    "stops = set(stopwords.words('english'))\n",
    "words1 = [i for i in letters1 if i not in stops]\n",
    "print words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'stuff', u'go', u'moment', u'mj', u've', u'start', u'listen', u'music', u'watch', u'odd', u'documentari', u'watch', u'wiz', u'watch', u'moonwalk', u'mayb', u'want', u'get', u'certain', u'insight', u'guy', u'thought', u'realli', u'cool', u'eighti', u'mayb', u'make', u'mind', u'whether', u'guilti', u'innoc', u'moonwalk', u'part', u'biographi', u'part', u'featur', u'film', u'rememb', u'go', u'see', u'cinema', u'origin', u'releas', u'subtl', u'messag', u'mj', u'feel', u'toward', u'press', u'also', u'obviou', u'messag', u'drug', u'bad', u'm', u'kay', u'visual', u'impress', u'cours', u'michael', u'jackson', u'unless', u'remot', u'like', u'mj', u'anyway', u'go', u'hate', u'find', u'bore', u'may', u'call', u'mj', u'egotist', u'consent', u'make', u'movi', u'mj', u'fan', u'would', u'say', u'made', u'fan', u'true', u'realli', u'nice', u'actual', u'featur', u'film', u'bit', u'final', u'start', u'minut', u'exclud', u'smooth', u'crimin', u'sequenc', u'joe', u'pesci', u'convinc', u'psychopath', u'power', u'drug', u'lord', u'want', u'mj', u'dead', u'bad', u'beyond', u'mj', u'overheard', u'plan', u'nah', u'joe', u'pesci', u'charact', u'rant', u'want', u'peopl', u'know', u'suppli', u'drug', u'etc', u'dunno', u'mayb', u'hate', u'mj', u'music', u'lot', u'cool', u'thing', u'like', u'mj', u'turn', u'car', u'robot', u'whole', u'speed', u'demon', u'sequenc', u'also', u'director', u'must', u'patienc', u'saint', u'came', u'film', u'kiddi', u'bad', u'sequenc', u'usual', u'director', u'hate', u'work', u'one', u'kid', u'let', u'alon', u'whole', u'bunch', u'perform', u'complex', u'danc', u'scene', u'bottom', u'line', u'movi', u'peopl', u'like', u'mj', u'one', u'level', u'anoth', u'think', u'peopl', u'stay', u'away', u'tri', u'give', u'wholesom', u'messag', u'iron', u'mj', u'bestest', u'buddi', u'movi', u'girl', u'michael', u'jackson', u'truli', u'one', u'talent', u'peopl', u'ever', u'grace', u'planet', u'guilti', u'well', u'attent', u've', u'gave', u'subject', u'hmmm', u'well', u'know', u'peopl', u'differ', u'behind', u'close', u'door', u'know', u'fact', u'either', u'extrem', u'nice', u'stupid', u'guy', u'one', u'sickest', u'liar', u'hope', u'latter']\n"
     ]
    }
   ],
   "source": [
    "# stemming \n",
    "porter = nltk.PorterStemmer()\n",
    "words1 = [porter.stem(w) for w in words1]\n",
    "print words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'stuff', u'go', u'moment', u'mj', u've', u'start', u'listen', u'music', u'watch', u'odd', u'documentari', u'watch', u'wiz', u'watch', u'moonwalk', u'mayb', u'want', u'get', u'certain', u'insight', u'guy', u'thought', u'realli', u'cool', u'eighti', u'mayb', u'make', u'mind', u'whether', u'guilti', u'innoc', u'moonwalk', u'part', u'biographi', u'part', u'featur', u'film', u'rememb', u'go', u'see', u'cinema', u'origin', u'releas', u'subtl', u'messag', u'mj', u'feel', u'toward', u'press', u'also', u'obviou', u'messag', u'drug', u'bad', u'm', u'kay', u'visual', u'impress', u'cours', u'michael', u'jackson', u'unless', u'remot', u'like', u'mj', u'anyway', u'go', u'hate', u'find', u'bore', u'may', u'call', u'mj', u'egotist', u'consent', u'make', u'movi', u'mj', u'fan', u'would', u'say', u'made', u'fan', u'true', u'realli', u'nice', u'actual', u'featur', u'film', u'bit', u'final', u'start', u'minut', u'exclud', u'smooth', u'crimin', u'sequenc', u'joe', u'pesci', u'convinc', u'psychopath', u'power', u'drug', u'lord', u'want', u'mj', u'dead', u'bad', u'beyond', u'mj', u'overheard', u'plan', u'nah', u'joe', u'pesci', u'charact', u'rant', u'want', u'peopl', u'know', u'suppli', u'drug', u'etc', u'dunno', u'mayb', u'hate', u'mj', u'music', u'lot', u'cool', u'thing', u'like', u'mj', u'turn', u'car', u'robot', u'whole', u'speed', u'demon', u'sequenc', u'also', u'director', u'must', u'patienc', u'saint', u'came', u'film', u'kiddi', u'bad', u'sequenc', u'usual', u'director', u'hate', u'work', u'one', u'kid', u'let', u'alon', u'whole', u'bunch', u'perform', u'complex', u'danc', u'scene', u'bottom', u'line', u'movi', u'peopl', u'like', u'mj', u'one', u'level', u'anoth', u'think', u'peopl', u'stay', u'away', u'tri', u'give', u'wholesom', u'messag', u'iron', u'mj', u'bestest', u'buddi', u'movi', u'girl', u'michael', u'jackson', u'truli', u'one', u'talent', u'peopl', u'ever', u'grace', u'planet', u'guilti', u'well', u'attent', u've', u'gave', u'subject', u'hmmm', u'well', u'know', u'peopl', u'differ', u'behind', u'close', u'door', u'know', u'fact', u'either', u'extrem', u'nice', u'stupid', u'guy', u'one', u'sickest', u'liar', u'hope', u'latter']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "words1 = [wnl.lemmatize(w) for w in words1]\n",
    "print words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj ve started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad m kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention ve gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "# joining back\n",
    "string1 = ' '.join(words1)\n",
    "print string1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff go moment mj ve start listen music watch odd documentari watch wiz watch moonwalk mayb want get certain insight guy thought realli cool eighti mayb make mind whether guilti innoc moonwalk part biographi part featur film rememb go see cinema origin releas subtl messag mj feel toward press also obviou messag drug bad m kay visual impress cours michael jackson unless remot like mj anyway go hate find bore may call mj egotist consent make movi mj fan would say made fan true realli nice actual featur film bit final start minut exclud smooth crimin sequenc joe pesci convinc psychopath power drug lord want mj dead bad beyond mj overheard plan nah joe pesci charact rant want peopl know suppli drug etc dunno mayb hate mj music lot cool thing like mj turn car robot whole speed demon sequenc also director must patienc saint came film kiddi bad sequenc usual director hate work one kid let alon whole bunch perform complex danc scene bottom line movi peopl like mj one level anoth think peopl stay away tri give wholesom messag iron mj bestest buddi movi girl michael jackson truli one talent peopl ever grace planet guilti well attent ve gave subject hmmm well know peopl differ behind close door know fact either extrem nice stupid guy one sickest liar hope latter\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# putting pieces together\n",
    "def review2words(review):\n",
    "    # 0. get text\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 1. remove punctuation and numbers\n",
    "    letters = re.sub(r'[^a-zA-Z]', ' ', review_text)\n",
    "    words = letters.split()\n",
    "    # 2. normalize i.e. lower\n",
    "    words = [w.lower() for w in words]\n",
    "    # 3. remove stop words\n",
    "    stops = set(stopwords.words('english'))\n",
    "    words = [w for w in words if w not in stops]\n",
    "    # 4. stem \n",
    "    # stemming is faster than lemmatization\n",
    "    porter = nltk.PorterStemmer()\n",
    "    words = [porter.stem(w) for w in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "print review2words(train['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 documents...\n",
      "processed 1000 documents...\n",
      "processed 2000 documents...\n",
      "processed 3000 documents...\n",
      "processed 4000 documents...\n",
      "processed 5000 documents...\n",
      "processed 6000 documents...\n",
      "processed 7000 documents...\n",
      "processed 8000 documents...\n",
      "processed 9000 documents...\n",
      "processed 10000 documents...\n",
      "processed 11000 documents...\n",
      "processed 12000 documents...\n",
      "processed 13000 documents...\n",
      "processed 14000 documents...\n",
      "processed 15000 documents...\n",
      "processed 16000 documents...\n",
      "processed 17000 documents...\n",
      "processed 18000 documents...\n",
      "processed 19000 documents...\n",
      "processed 20000 documents...\n",
      "processed 21000 documents...\n",
      "processed 22000 documents...\n",
      "processed 23000 documents...\n",
      "processed 24000 documents...\n"
     ]
    }
   ],
   "source": [
    "n_reviews = train['review'].size\n",
    "cleaned_reviews = []\n",
    "for i in xrange(n_reviews):\n",
    "    if i%1000 == 0:\n",
    "        print \"processed {0} documents...\".format(i)\n",
    "    cleaned_reviews.append(review2words(train['review'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "train_features = vectorizer.fit_transform(cleaned_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In RF: A sparse matrix was passed, but dense data is required. \n",
    "# Use X.toarray() to convert to a dense numpy array.\n",
    "\n",
    "train_features = train_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# now that the vectorizer is trained. get features list\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "print len(vocabulary)\n",
    "# get distribution of each word\n",
    "dist = np.sum(train_features, axis=0)\n",
    "\n",
    "#for w, d in zip(vocabulary, dist):\n",
    "#    print w, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "\n",
    "forest = forest.fit( train_features, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Word2Vec \n",
    "## \n",
    "def review2words2(review, remove_stopwords = False):\n",
    "    # 0. get text\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 1. remove punctuation and numbers\n",
    "    letters = re.sub(r'[^a-zA-Z]', ' ', review_text)\n",
    "    words = letters.split()\n",
    "    # 2. normalize i.e. lower\n",
    "    words = [w.lower() for w in words]\n",
    "    # 3. remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if w not in stops]\n",
    "    # 4. stem \n",
    "    # stemming is faster than lemmatization\n",
    "    porter = nltk.PorterStemmer()\n",
    "    words = [porter.stem(w) for w in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read more data\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.tokenize.punkt.PunktSentenceTokenizer at 0x26a12240>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec expects single sentences, each one as a list of words. \n",
    "# Use nltk.punkt to split a paragraph into sentences.\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review2sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Function to split review into sentences. \n",
    "    # Each sentence is represented as a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    # 2. Loop over each sentence.\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review2words2(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "# \n",
    "# to train Word2Vec it is better not to remove stop words because \n",
    "# the algorithm relies on the broader context of the sentence in \n",
    "# order to produce high-quality word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:182: UserWarning: \"... ...\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:182: UserWarning: \"....\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:182: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:182: UserWarning: \".. .\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hong Xu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "# Now we can apply this function to prepare our data for input to Word2Vec \n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review2sentences(review.decode('utf8'), tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review2sentences(review.decode('utf8'), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "[[u'stuff', u'go', u'moment', u'mj', u've', u'start', u'listen', u'music', u'watch', u'odd', u'documentari', u'watch', u'wiz', u'watch', u'moonwalk'], [u'mayb', u'want', u'get', u'certain', u'insight', u'guy', u'thought', u'realli', u'cool', u'eighti', u'mayb', u'make', u'mind', u'whether', u'guilti', u'innoc'], [u'moonwalk', u'part', u'biographi', u'part', u'featur', u'film', u'rememb', u'go', u'see', u'cinema', u'origin', u'releas'], [u'subtl', u'messag', u'mj', u'feel', u'toward', u'press', u'also', u'obviou', u'messag', u'drug', u'bad', u'm', u'kay', u'visual', u'impress', u'cours', u'michael', u'jackson', u'unless', u'remot', u'like', u'mj', u'anyway', u'go', u'hate', u'find', u'bore'], [u'may', u'call', u'mj', u'egotist', u'consent', u'make', u'movi', u'mj', u'fan', u'would', u'say', u'made', u'fan', u'true', u'realli', u'nice', u'actual', u'featur', u'film', u'bit', u'final', u'start', u'minut', u'exclud', u'smooth', u'crimin', u'sequenc', u'joe', u'pesci', u'convinc', u'psychopath', u'power', u'drug', u'lord']]\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)\n",
    "print sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(sklearn.base.BaseEstimator, VectorizerMixin)\n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.coo_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  lowercase : boolean, True by default\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if `tokenize == 'word'`. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, optional, 1.0 by default\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, optional, 1 by default\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : optional, None by default\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : boolean, False by default.\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  `vocabulary_` : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  `stop_words_` : set\n",
      " |      Terms that were ignored because\n",
      " |      they occurred in either too many\n",
      " |      (`max_df`) or in too few (`min_df`) documents.\n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input=u'content', encoding=u'utf-8', charset=None, decode_error=u'strict', charset_error=None, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer=u'word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>)\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep: boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The former have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from VectorizerMixin:\n",
      " |  \n",
      " |  fixed_vocabulary\n",
      " |      DEPRECATED: The `fixed_vocabulary` attribute is deprecated and will be removed in 0.18.  Please use `fixed_vocabulary_` instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
